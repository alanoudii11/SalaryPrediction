---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
Installing the packages:
```{r}
install.packages("outliers")
install.packages("scatterplot3d") 
install.packages("dplyr")
install.packages("ggplot2")
install.packages("colorspace")

```

# Reading libraries 
```{r}
library(ggplot2)
library(outliers)
library(scatterplot3d)
library(dplyr)
library(readr)

```
 


## load the data
```{r}
dataset <- read_csv("salary.csv")

```



1- General information about the Dataset

#Sample of raw dataset and summary 
```{r}
head(dataset)
summary(dataset)
```

First and foremost, we will remove a few of our columns. We will remove education, capital.gain, and capital.loss. Education to avoid redundancy, as we believe education.num is adequate on its own. We will remove capital.loss and capital.gain due to low variance, with overwhelming majority of rows being zero.
```{r}
dataset<- dataset[-c(4, 11,12)]
print(dataset)
```


#Encoding the data: 
  The reason we performed encoding on our dataset is because it not only preserves the integrity of our data, but also because machine learning algorithms work with numerical values. 
```{r}
dataset$workclass <-  as.integer(factor(dataset$workclass))
dataset$`marital-status` <-  as.integer(factor(dataset$`marital-status`))
dataset$occupation <-  as.integer(factor(dataset$occupation))
dataset$relationship <-  as.integer(factor(dataset$workclass))
dataset$race <-  as.integer(factor(dataset$race))
dataset$sex <-  as.integer(factor(dataset$sex))
dataset$`native-country` <-  as.integer(factor(dataset$`native-country`))
print(dataset)
```

2- Data Preprocessing

2.1- Data cleaning:
  To clean the data of the dataset, we are going to detect and remove outliers, fill in missing values, and replace negative values.

  *Removing missing values:
```{r}
sum(is.na(dataset))

```
  We noticed that there is a question mark in some values although we checked the missing values in previous part and the resault was 0 so we are going to clean them by deleting the row
```{r}
dataset <- dataset[!apply(dataset == "?", 1, any), ]
dataset <- na.omit(dataset)
print(dataset)
```
 
  *Determining and removing outliers
We will detect outliers in all numerical attributes and remove them.


```{r}
OutAge = outlier(dataset$age, logical =TRUE)
sum(OutAge)
Find_outlier = which(OutAge ==TRUE, arr.ind = TRUE)

####
Outfnlwgt = outlier(dataset$fnlwgt, logical =TRUE)
sum(Outfnlwgt)
Find_outlier = which(Outfnlwgt ==TRUE, arr.ind = TRUE)

###
Outeducation = outlier(dataset$`education-num`, logical =TRUE)
sum(Outeducation)
Find_outlier = which(Outeducation ==TRUE, arr.ind = TRUE)

##

Outhours = outlier(dataset$`hours-per-week`, logical =TRUE)
sum(Outhours)
Find_outlier = which(Outhours ==TRUE, arr.ind = TRUE)



#Remove outlier
dataset= dataset[-Find_outlier,]

```
Printing the dataset after cleaning the data
```{r}
print(dataset)
```

Create a new column called num_salary to transform the binary attribute into numerical to use it better as it it the class label
```{r}
dataset$num_salary <- ifelse(dataset$salary == ">50K", 1, 0)

```

2.4- Data transformation 

Normalization
Normalization is applied as part of data preparation for machine learning. Its purpose is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.

Normalize attributes: age, fnlwgt

```{r}
dataWithoutNormalization <- dataset

#Define function normalize().
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
#Define function Z_normalize().
Z_normalize <- function(x) {return ((x - mean(x)) / sd(x))}

#Call normalize funcrtion 
dataset$age<-normalize(dataWithoutNormalization$age)


#Call Z_normalize funcrtion 
dataset$age<-Z_normalize(dataWithoutNormalization$age)


######################################

#Call normalize funcrtion 
dataset$fnlwgt<-normalize(dataWithoutNormalization$fnlwgt)


#Call Z_normalize funcrtion 
dataset$fnlwgt<-Z_normalize(dataWithoutNormalization$fnlwgt)

print(dataset) # data after normalization



```
3- Data Summary and Visualixation

3.1- Data summary after preprocessing:

```{r}
# number of rows
nrow(dataset)
```
There are 32561 rows in the dataset after preprocessing.

```{r}
# number of columns 
ncol(dataset)
```
There are 12 columns after preprocessing.

```{r}
# Dimensionalitly 
 dim(dataset)
```
There are 32476 rows and 12 columns in the dataset after preprocessing

  *Show the column Names of Clean (Preprocessed) Dataset
```{r}
names(dataset)
```


  *Provide a summary of the Data 
```{r}
summary(dataset$age)
summary(dataset$`hours-per-week`)
summary(dataset$occupation)
summary(dataset$sex)
summary(dataset$salary)
```
  *Show the variance of age and hours.per.week
```{r}
var(dataset$age)
var(dataset$`hours-per-week`)
```

Create a density plot for the age variable
```{r}
ggplot(data = dataset, aes(x = age)) + 
  geom_density()
```

# Plotting:
  *Histograms
### Histogram of the age variable
```{r}
ggplot(dataset, aes(x = age)) +
  geom_histogram()
```
### Histogram of the num_salary variable where the 1 represents salary >=50k and 0 represents salary <50k
```{r}
ggplot(dataset, aes(x = num_salary)) +
  geom_histogram()

```
### Histogram of the hours-per-week variable
```{r}
ggplot(dataset, aes(x =`hours-per-week`)) +
  geom_histogram()

```
### Histogram of the fnlwgt variable
```{r}
ggplot(dataset, aes(x = fnlwgt)) +
  geom_histogram()
```

  *Barplots
### Bar plot of the workclass variable
```{r}
ggplot(dataset, aes(x = workclass)) + geom_bar()

```
### Bar plot of the occupation variable
```{r}
ggplot(dataset, aes(x = occupation)) + geom_bar()
```
### Bar plot of the relationship variable
```{r}
ggplot(dataset, aes(x = relationship)) + geom_bar()
```
### Bar plot of the sex variable
```{r}
ggplot(dataset, aes(x = sex)) +geom_bar()
```


### Bar plot of the salary variable
```{r}
ggplot(dataset, aes(x = salary)) + geom_bar()

```
 
  *Boxplots
```{r}
ggplot(dataset, aes(x = salary, y = `hours-per-week`)) +
  geom_boxplot() +
  labs(title = "Box Plot of Salary and Hours per Week", x = "Salary", y = "Hours per Week")

```

```{r}
ggplot(dataset, aes(x = salary, y = age)) +
  geom_boxplot() +
  labs(title = "Box Plot of Age and Salary", x = "salary", y = "age")
```

  *Scatterplot
```{r}
scatterplot3d(dataset$num_salary,dataset$age, dataset$`hours-per-week`)
```


  *Pie chart
```{r}
tab <- dataset$salary %>% table()
precentages <- tab %>% prop.table() %>% round(3) * 100 
txt <- paste0(names(tab), '\n', precentages, '%') 
pie(tab, labels=txt)
```

##CLUSTERING

First, we install the needed packages and libraries.

```{r}
install.packages("dplyr")
install.packages("tidyverse")
install.packages("e1071")
install.packages("cluster")
install.packages("factoextra")
install.packages("fpc")
library(dplyr)
library(tidyverse)
library(e1071)
library(cluster)
library(factoextra)
library(fpc)
```


Take a sample of the first 10,000 rows in the dataset. 
```{r}

sampleDataset = dataset[1:10000,]

```

Preparing the data before clustering:

We will first start by deleting the class label, as clustering is an unsupervised learning.

```{r}
data_clustring <- subset(sampleDataset, select = -salary)
data_clustring <- subset(data_clustring, select = -num_salary)
print(data_clustring)

```

scaling the data

```{r} 

data_clustring <- scale(data_clustring)

```


#Data after preparing 

```{r}
summary(data_clustring)
str(data_clustring)

```

  It is a good idea to experiment with different values of k to determine the best value for our dataset. By using various methods, such as the silhouette coefficient or the gap statistic, to evaluate the quality of different clustering solutions and select the one that best represents the underlying structure of the data.

#Perform K-means clustering (for k = 2 clusters)

```{r}
kmeans_model <- kmeans(data_clustring,2 , nstart = 50)
kmeans_model
```

# Extract the cluster assignments

```{r}
clusters <- kmeans_model$cluster
```

# Visualize the clusters

```{r}
fviz_cluster(list(data = data_clustring, cluster = clusters), geom = "point", main= "cluster plot for k=2")

```

# Calculating the avg silhouette

```{r}
avg_sil=silhouette (kmeans_model$cluster, dist(data_clustring))
fviz_silhouette(avg_sil)
```
The WCSS is a measure of the compactness of the clusters, and it represents the total distance of all data points within their respective clusters. A lower WCSS indicates that the clusters are more compact and well-separated, while a higher WCSS suggests that the clusters are more spread out and less distinct.

# Total within-cluster sum of square

```{r}
kmeans_model$tot.withinss
```
Based on the graph and since Total within-cluster sum of square is 95968.12, the clusters appear to be well-defined and separated. The data points in each cluster are closely grouped together, and there is a clear gap between the two clusters. This suggests that the kmeans_model$tot.withinss value of 95967.99 is relatively low, indicating that the clusters are compact and well-separated.



# Calculate BCubed precision and recall

# First we set the ground truth

```{r}
ground_truth <- c(sampleDataset$salary)
```

# Setting the functions for calculating the BCubed precision and recall

```{r}
BCubed_Precision <- function(clusters, ground_truth) {
  n <- length(clusters)
  precision <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
  }
  precision <- precision / n
  return(precision)
}

BCubed_Recall <- function(clusters, ground_truth) {
  n <- length(clusters)
  recall <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
  }
  recall <- recall / n
  return(recall)
}



```

# Calculate BCubed precision and recall using the cluster assignments and ground truth

```{r}
precision <- BCubed_Precision(clusters, ground_truth)
recall <- BCubed_Recall(clusters, ground_truth)
```

# View the calculated precision and recall

```{r}
print(precision)
print(recall)
```

# Perform K-means clustering (for k = 3 clusters)

```{r}
kmeans_model <- kmeans(data_clustring,3 , nstart = 50)
kmeans_model
```

# Extract the cluster assignments

```{r}
clusters <- kmeans_model$cluster
```

# Visualize the clusters

```{r}
fviz_cluster(list(data = data_clustring, cluster = clusters), geom = "point", main= "cluster plot for k=3")

```

# calculating the avg silhouette

```{r}
avg_sil=silhouette (kmeans_model$cluster, dist(data_clustring))
fviz_silhouette(avg_sil)
```

# Total within-cluster sum of square

```{r}
kmeans_model$tot.withinss
```

The kmeans_model$tot.withinss value of 86497.36 indicates that the two clusters are very compact and well-separated. The data points in each cluster are tightly grouped together, and there is a large gap between the two clusters. This suggests that the K-means clustering algorithm has effectively captured the underlying structure of the data and produced two meaningful clusters.


# Calculate BCubed precision and recall

# First we set the ground truth

```{r}
ground_truth <- c(sampleDataset$salary)
```

# Setting the functions for calculating the BCubed precision and recall

```{r}
BCubed_Precision <- function(clusters, ground_truth) {
  n <- length(clusters)
  precision <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
  }
  precision <- precision / n
  return(precision)
}

BCubed_Recall <- function(clusters, ground_truth) {
  n <- length(clusters)
  recall <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
  }
  recall <- recall / n
  return(recall)
}



```

# Calculate BCubed precision and recall using the cluster assignments and ground truth

```{r}
precision <- BCubed_Precision(clusters, ground_truth)
recall <- BCubed_Recall(clusters, ground_truth)
```

# View the calculated precision and recall

```{r}
print(precision)
print(recall)
```

# Perform K-means clustering (for k = 4 clusters)

```{r}
kmeans_model <- kmeans(data_clustring,4 , nstart = 50)
kmeans_model
```

# Extract the cluster assignments

```{r}
clusters <- kmeans_model$cluster
```

# Visualize the clusters

```{r}
fviz_cluster(list(data = data_clustring, cluster = clusters), geom = "point", main= "cluster plot for k=4")

```

# Calculating the avg silhouette

```{r}
avg_sil=silhouette (kmeans_model$cluster, dist(data_clustring))
fviz_silhouette(avg_sil)
```

# Total within-cluster sum of square

```{r}
kmeans_model$tot.withinss
```

Based on the graph provided, the kmeans_model$tot.withinss of 78835.45 indicates that the clusters are compact and well-separated. The data points in each cluster are tightly grouped together, and there is a clear gap between the two clusters. This is evident in the fact that the two clusters are clearly distinct in the plot, with little overlap between them.


# Calculate BCubed precision and recall

# First we set the ground truth

```{r}
ground_truth <- c(sampleDataset$salary)
```

# Setting the functions for calculating the BCubed precision and recall

```{r}
BCubed_Precision <- function(clusters, ground_truth) {
  n <- length(clusters)
  precision <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    precision <- precision + sum(same_cluster & same_ground_truth) / sum(same_cluster)
  }
  precision <- precision / n
  return(precision)
}

BCubed_Recall <- function(clusters, ground_truth) {
  n <- length(clusters)
  recall <- 0
  for (i in 1:n) {
    same_cluster <- clusters == clusters[i]
    same_ground_truth <- ground_truth == ground_truth[i]
    recall <- recall + sum(same_cluster & same_ground_truth) / sum(same_ground_truth)
  }
  recall <- recall / n
  return(recall)
}



```

# Calculate BCubed precision and recall using the cluster assignments and ground truth

```{r}
precision <- BCubed_Precision(clusters, ground_truth)
recall <- BCubed_Recall(clusters, ground_truth)
```

# View the calculated precision and recall

```{r}
print(precision)
print(recall)
```



K=2
Average Silhouette Width: 0.23
Total Within-Cluster Sum of Squares: 95968.12
Precision: 0.637
Recall: 0.7446


K=3
Average Silhouette Width: 0.14
Total Within-Cluster Sum of Squares: 86497.36
Precision: 0.6753
Recall: 0.4412


K=4
Average Silhouette Width: 0.15
Total Within-Cluster Sum of Squares: 78835.45
Precision: 0.673
Recall: 0.3950


Analysis:

Average Silhouette Width: K=2 shows the highest silhouette width (0.23), indicating better-defined clusters in comparison to K=3 and K=4.
Total Within-Cluster Sum of Squares: As the number of clusters increases, the total within-cluster sum of squares decreases, suggesting better compactness or tighter clustering.

Precision and Recall: While precision remains relatively consistent, recall shows a decline as the number of clusters increases. This suggests a trade-off between precision and recall, where a higher number of clusters might lead to better precision but lower recall due to more fragmented or smaller clusters.

Overall Comparison:

K=2 seems to strike a balance, offering a relatively high silhouette width and higher total within-cluster sum of squares, indicating well-separated and compact clusters

K=3 demonstrates a fair silhouette width and balanced precision but lower recall; the clusters might be less well-defined compared to K=2.

K=4 shows high precision but at the expense of lower recall, possibly due to the fragmentation of data into smaller clusters. 

# Comparison of WCSS

Clustering Result 1: kmeans_model$tot.withinss = 95968.12

The clusters appear to be well-defined and separated in the graph.
The WCSS value of 95968.12 is moderate, suggesting that the clusters are neither overly compact nor overly spread out.

Clustering Result 2: kmeans_model$tot.withinss = 86497.36

The clusters appear to be compact and well-separated in the graph.
The WCSS value of 86497.36 is lower than the first clustering result, indicating that the clusters are more compact in this case.

Clustering Result 3: kmeans_model$tot.withinss = 78835.45

The clusters appear to be very compact and well-separated in the graph.
The WCSS value of 78835.45 is the lowest of the three clustering results, indicating that the clusters are the most compact in this case.


Recommendation:

Based on the WCSS values and the visual inspection of the clusters in the graphs, we would suggest that Clustering Result 3 (kmeans_model$tot.withinss = 78835.45) is the best of the three clustering solutions. The clusters in this result are the most compact and well-separated, and the WCSS value is significantly lower than the other two results. This suggests that the K-means algorithm has effectively captured the underlying structure of the data and produced two meaningful clusters.



```{r}
silhouette_score <- function(k){ 
km <- kmeans(data_clustring, centers = k,nstart=50) # if centers is a number, how many random sets should be chosen?
ss <- silhouette(km$cluster, dist(data_clustring))
sil<- mean(ss[, 3])
return(sil)
}
```

# k cluster range from 2 to 10

```{r}
k <- 2:10
```

# call function fore k value

```{r}
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
```

The Silhouette method graph shows how the average silhouette coefficient changes as the number of clusters (k) increases. The optimal number of clusters is often chosen as the value of k that corresponds to the highest average silhouette coefficient. However, it is important to note that the highest average silhouette coefficient may not necessarily indicate the best clustering solution. For example, if the average silhouette coefficient is high for all values of k, it may suggest that the data is not well-clustered at all.

# Finding the optimal number of clusters applying Silhouette method

```{r}
fviz_nbclust(data_clustring, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

# Finding the optimal number of clusters applying Elbow method

```{r}
fviz_nbclust(data_clustring, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

```

The Elbow method graph shows that the WCSS decreases rapidly as the number of clusters (k) increases. However, the rate of decrease slows down significantly after k=2. This suggests that k=2 is likely to be the optimal number of clusters, as it minimizes the WCSS without overfitting the data




##CLASSIFICATION

Classification uses supervised learning, meaning it uses labeled datasets to train models to classify or predict data. We will be applying three different algorithms: ID3, C4.5, and CART.

  *ID3 Algorithm:

  Taking a sample of the first 5000 rows in our dataset, we then divided them into two: a training set and a testing set. For the training model, we used 3 different sizes; 60%, 70% and 80%. 
  
  After constructing our three decision trees using ID3 algorithm, we can see that the attribute ‘education.num’ was the root node in all three of our trees. This means that ‘education.num’ has the highest information gain. 
  
  Upon using the confusion matrix and observing the accuracy of our models, we’ve noticed that all three showed a high accuracy. However, because our dataset is imbalanced, we will focus on the sensitivity and specificity to determine the performance of our models. 

In the 60/40 model, not only was accuracy 98%, and the confusion matrix shows high accuracy, sensitivity, and specificity. 
```{r}
install.packages("party",  repos = "https://cran.r-project.org")
library(party)
install.packages("caret",  repos = "https://cran.r-project.org")
library(caret)

#evaluation: 60% training, 40% testing

sampleDataset=dataset[1:10000, ]
set.seed(1234)
ind <- sample(2, nrow(sampleDataset), replace = TRUE, prob = c(0.6, 0.4))
trainingData <- sampleDataset[ind == 1,]
testingData <- sampleDataset[ind == 2,]

myFormula <- num_salary ~ age + workclass + fnlwgt +'education-num' +'marital-status' + 'hours.per.week' + occupation + relationship + race + sex  + native-country
salary_Tree <- ctree(myFormula, data = trainingData)
plot(salary_Tree, type = "simple")
plot(salary_Tree)
print(salary_Tree)

# Convert the response variable to a factor
testingData$num_salary <- as.factor(testingData$num_salary)

# Data Prediction
testPrediction <- predict(salary_Tree, newdata = testingData)

# Ensure testPrediction has the same levels as testingData$num_salary
testPrediction <- factor(testPrediction, levels = levels(testingData$num_salary))

# Create Confusion Matrix
results <- confusionMatrix(as.factor(testPrediction), testingData$num_salary)
print(results)

```


The 70/30 model, much like the 60/40, gave us a high accuracy of 98%. Like the previous model, it performs well, giving us high accuracy, sensitivity, and specificity. 
```{r}
#evaluation: 70% training, 30% testing

sampleDataset=dataset[1:10000, ]
set.seed(1234)
ind <- sample(2, nrow(sampleDataset), replace = TRUE, prob = c(0.7, 0.3))
trainingData <- sampleDataset[ind == 1,]
testingData <- sampleDataset[ind == 2,]

myFormula <- num_salary ~ age + workclass + fnlwgt +education.num +marital.status + hours.per.week + occupation + relationship + race + sex  + native.country
salary_Tree <- ctree(myFormula, data = trainingData)

plot(salary_Tree, type = "simple")
plot(salary_Tree)
print(salary_Tree)

# Convert the response variable to a factor
testingData$num_salary <- as.factor(testingData$num_salary)

# Data Prediction
testPrediction <- predict(salary_Tree, newdata = testingData)

# Ensure testPrediction has the same levels as testingData$num_salary
testPrediction <- factor(testPrediction, levels = levels(testingData$num_salary))

# Create Confusion Matrix
results <- confusionMatrix(as.factor(testPrediction), testingData$num_salary)
print(results)


```


Our last model, the 80/20 model, gives us similar results. The accuracy is 97%, and high sensitivity and specificity. 


```{r}

##evaluation: 80% training, 20% testing
sampleDataset=dataset[1:10000, ]
set.seed(1234)
ind <- sample(2, nrow(sampleDataset), replace = TRUE, prob = c(0.8, 0.2))
trainingData <- sampleDataset[ind == 1,]
testingData <- sampleDataset[ind == 2,]

myFormula <- num_salary ~ age + workclass + fnlwgt +education.num +marital.status + hours.per.week + occupation + relationship + race + sex + native.country
salary_Tree <- ctree(myFormula, data = trainingData)

plot(salary_Tree, type = "simple")
plot(salary_Tree)
print(salary_Tree)

# Convert the response variable to a factor
testingData$num_salary <- as.factor(testingData$num_salary)

# Data Prediction
testPrediction <- predict(salary_Tree, newdata = testingData)

# Ensure testPrediction has the same levels as testingData$num_salary
testPrediction <- factor(testPrediction, levels = levels(testingData$num_salary))

# Create Confusion Matrix
results <- confusionMatrix(as.factor(testPrediction), testingData$num_salary)
print(results)


```


Ananlysis: While the accuracy for all our models are high, as mentioned before, we will be looking instead at the sensitivity and specificity to determine which model was the best. All three of our models have high recall and specificity, but the model with the best performance was the 60/40 model with 98% sensitivity and 98% specificity. 


  *CART Algorithm
  
  The cp (complexity parameter) in the CART alogrithm determines the complexity of the tree. The smaller the cp, the more complex the tree becomes. We will try 3 different cp values; 0.5, 0.01, 0.008.


After setting the cp value to 0.5, we can see that our tree has only produced one node with no split. Because of this, the variable importance is NULL. The tree does not need to be pruned. 

```{r}
over50K=ifelse(dataset$num_salary==1, "Yes", "No")
dataset=data.frame(dataset, over50K)
library(rpart)
library(rpart.plot)

#Remove salary, num_salary
dataset <- subset(dataset, select = -salary)
dataset <- subset(dataset, select = -num_salary)

View(dataset.salary)

dataset.salary$over50K = as.factor(dataset$over50K)
class(dataset.salary$over50K)

set.seed(1234)
train = seq_len(10000)
dataset.train=dataset.salary[train,]
dataset.test=dataset.salary[-train,]

over50K.test=over50K[-train]

#CP=0.5

fit.tree = rpart(over50K ~ ., data=dataset.train, method = "class", cp=0.5)
fit.tree
rpart.plot(fit.tree)

# Checking the order of variable importance
fit.tree$variable.importance
pred.tree = predict(fit.tree, dataset.test, type = "class")
table(pred.tree,over50K.test)
#plotcp(fit.tree)
printcp(fit.tree)


```

  After setting the cp to 0.01, the tree has a root and several terminal nodes. The most important variables in this tree is marital.status. We’ve noticed that even after pruning, the tree structure has not changed. And the accuracy snd sensitivity also did not change.

```{r}
#CP=0.01

fit.tree = rpart(over50K ~ ., data=dataset.train, method = "class", cp=0.01)
fit.tree
rpart.plot(fit.tree)

# Checking the order of variable importance
fit.tree$variable.importance
pred.tree = predict(fit.tree, dataset.test, type = "class")
confMatBeforePruning=table(pred.tree,over50K.test)
#plotcp(fit.tree)
printcp(fit.tree)

#calculate accuracy:
accuracy <- sum(diag(confMatBeforePruning))/sum(confMatBeforePruning)
accuracy

#calculate sensitivity:
tp <- confMatBeforePruning[2, 2] 
fn <- confMatBeforePruning[2, 1]
sensitivity <- tp / (tp + fn)
sensitivity

# Explicitly request the lowest cp value
fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"]
bestcp <-fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"]
pruned.tree <- prune(fit.tree, cp = bestcp)
rpart.plot(pruned.tree)

# Alternate specification 
pred.prune = predict(pruned.tree, dataset.test, type="class")
confMatAfterPruning=table(pred.prune, over50K.test)

#calculate accuracy:
accuracy <- sum(diag(confMatAfterPruning))/sum(confMatAfterPruning)
accuracy

#calculate sensitivity:
tp <- confMatAfterPruning[2, 2] 
fn <- confMatAfterPruning[2, 1]
sensitivity <- tp / (tp + fn)
sensitivity
```


  After setting the cp to 0.0008, the tree has a root and several terminal nodes. The most important variables in this tree, like the previous tree was the marital.status. We’ve noticed that even after pruning, the tree structure has changed a little, however, the accuracy and sensitivity also did not change by much. 

```{r}
#CP=0.0008

fit.tree = rpart(over50K ~ ., data=dataset.train, method = "class", cp=0.0008)
fit.tree
rpart.plot(fit.tree)

# Checking the order of variable importance
fit.tree$variable.importance
pred.tree = predict(fit.tree, dataset.test, type = "class")
confMatBeforePruning=table(pred.tree,over50K.test)
#plotcp(fit.tree)
printcp(fit.tree)

#calculate accuracy:
accuracy <- sum(diag(confMatBeforePruning))/sum(confMatBeforePruning)
accuracy

#calculate sensitivity:
tp <- confMatBeforePruning[2, 2] 
fn <- confMatBeforePruning[2, 1]
sensitivity <- tp / (tp + fn)
sensitivity

# Explicitly request the lowest cp value
fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"]
bestcp <-fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"]
pruned.tree <- prune(fit.tree, cp = bestcp)
rpart.plot(pruned.tree)

# Alternate specification 
pred.prune = predict(pruned.tree, dataset.test, type="class")
confMatAfterPruning=table(pred.prune, over50K.test)

#calculate accuracy:
accuracy <- sum(diag(confMatAfterPruning))/sum(confMatAfterPruning)
accuracy

#calculate sensitivity:
tp <- confMatAfterPruning[2, 2] 
fn <- confMatAfterPruning[2, 1]
sensitivity <- tp / (tp + fn)
sensitivity
```



  *C4.5 Algorithm
  k-folds affect bias and variance of a model’s performance. The higher the folds are, the higher the bias and lower the variance. We will be testing 3, 5, and 10 folds and evaluating the performance of each one. 

  In 3-fold training set, the accuracy and sensitivity were high, but the specificity was low, meaning the model is not as effective when predicting the instances where the salary is higher than 50K. 

```{r}

library(RWeka)
install.packages("partykit",  repos = "https://cran.r-project.org")

#Note: Remove num_salary & over50K first
dataset <- subset(dataset, select = -salary)
dataset <- subset(dataset, select = -over50K)

sampleDataset <- dataset[1:10000, ]
set.seed(1234)
#First, we train the model. We will start with a 3-fold training set. 
ctrl <- trainControl(method = "cv", number = 3)

C45Fit <- train(salary ~ ., method = "J48", data = sampleDataset,
                tuneLength = 5,
                trControl = ctrl)

final_model <- C45Fit$finalModel
plot(final_model)


predictions <- predict(final_model, newdata = sampleDataset)

predictions<-as.factor(predictions)
sampleDataset$salary<-as.factor(sampleDataset$salary)

# Create a confusion matrix
conf_matrix <- confusionMatrix(predictions, sampleDataset$salary)

# Print the confusion matrix
print(conf_matrix)
```


The 5-fold training set produced very similar results to the 3-fold set, with high accuracy and sensitivity, but low specificity. 
```{r}
##5-fold training set
set.seed(1234)
ctrl <- trainControl(method = "cv", number = 5)

C45Fit <- train(salary ~ ., method = "J48", data = sampleDataset,
                tuneLength = 5,
                trControl = ctrl)

final_model <- C45Fit$finalModel
plot(final_model)

predictions <- predict(final_model, newdata = sampleDataset)

predictions<-as.factor(predictions)
sampleDataset$salary<-as.factor(sampleDataset$salary)

# Create a confusion matrix
conf_matrix <- confusionMatrix(predictions, sampleDataset$salary)

# Print the confusion matrix
print(conf_matrix)
```



The 10-fold training set had similar results, but with the highest sensitivity out of the three models. 
```{r}
##10-fold training set
set.seed(1234)
ctrl <- trainControl(method = "cv", number = 10)

C45Fit <- train(salary ~ ., method = "J48", data = sampleDataset,
                tuneLength = 5,
                trControl = ctrl)

final_model <- C45Fit$finalModel
plot(final_model)

predictions <- predict(final_model, newdata = sampleDataset)

predictions<-as.factor(predictions)
sampleDataset$salary<-as.factor(sampleDataset$salary)

# Create a confusion matrix
conf_matrix <- confusionMatrix(predictions, sampleDataset$salary)

# Print the confusion matrix
print(conf_matrix)


```

Conclusion: Overall, while all three algorithms had a high accuracy, if we take into account the sensitivity and specificity, we can see that the best performing algorithm was the ID3 algorithm.